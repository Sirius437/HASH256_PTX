// SHA256 PTX Kernel - Auto-generated with all 64 rounds
// Generated by generate_sha256_ptx.py

.version 8.7
.target sm_120
.address_size 64

// SHA256 K constants
.const .align 4 .b32 K[64] = {
    0x428a2f98, 0x71374491, 0xb5c0fbcf, 0xe9b5dba5,
    0x3956c25b, 0x59f111f1, 0x923f82a4, 0xab1c5ed5,
    0xd807aa98, 0x12835b01, 0x243185be, 0x550c7dc3,
    0x72be5d74, 0x80deb1fe, 0x9bdc06a7, 0xc19bf174,
    0xe49b69c1, 0xefbe4786, 0x0fc19dc6, 0x240ca1cc,
    0x2de92c6f, 0x4a7484aa, 0x5cb0a9dc, 0x76f988da,
    0x983e5152, 0xa831c66d, 0xb00327c8, 0xbf597fc7,
    0xc6e00bf3, 0xd5a79147, 0x06ca6351, 0x14292967,
    0x27b70a85, 0x2e1b2138, 0x4d2c6dfc, 0x53380d13,
    0x650a7354, 0x766a0abb, 0x81c2c92e, 0x92722c85,
    0xa2bfe8a1, 0xa81a664b, 0xc24b8b70, 0xc76c51a3,
    0xd192e819, 0xd6990624, 0xf40e3585, 0x106aa070,
    0x19a4c116, 0x1e376c08, 0x2748774c, 0x34b0bcb5,
    0x391c0cb3, 0x4ed8aa4a, 0x5b9cca4f, 0x682e6ff3,
    0x748f82ee, 0x78a5636f, 0x84c87814, 0x8cc70208,
    0x90befffa, 0xa4506ceb, 0xbef9a3f7, 0xc67178f2
};

.visible .entry sha256_kernel(
    .param .u64 param_input,
    .param .u64 param_output,
    .param .u32 param_num_keys
)
{
    .reg .b32   %r<100>;
    .reg .b64   %rd<10>;
    .reg .pred  %p<10>;
    
    .reg .b32   %thread_id;
    .reg .b64   %input_ptr, %output_ptr;
    .reg .b64   %input_base, %output_base;
    
    // SHA256 state
    .reg .b32   %a, %b, %c, %d, %e, %f, %g, %h;
    .reg .b32   %h0, %h1, %h2, %h3, %h4, %h5, %h6, %h7;
    
    // Message schedule
    .reg .b32   %w0, %w1, %w2, %w3, %w4, %w5, %w6, %w7;
    .reg .b32   %w8, %w9, %w10, %w11, %w12, %w13, %w14, %w15;
    
    // Temporaries
    .reg .b32   %t1, %t2, %ch, %maj;
    .reg .b32   %s0, %s1;  // message schedule sigma (lowercase)
    .reg .b32   %S0, %S1;  // round Sigma (uppercase)
    .reg .b32   %k_val, %w_val;
    
    // Thread ID calculation
    mov.u32     %r0, %ctaid.x;
    mov.u32     %r1, %ntid.x;
    mov.u32     %r2, %tid.x;
    mad.lo.s32  %thread_id, %r0, %r1, %r2;
    
    // Load parameters
    ld.param.u64    %input_base, [param_input];
    ld.param.u64    %output_base, [param_output];
    ld.param.u32    %r3, [param_num_keys];
    
    // Bounds check
    setp.ge.u32     %p0, %thread_id, %r3;
    @%p0 bra        END;
    
    // Convert to global addresses
    cvta.to.global.u64  %input_base, %input_base;
    cvta.to.global.u64  %output_base, %output_base;
    
    // Calculate pointers
    mul.wide.u32    %rd0, %thread_id, 33;
    add.u64         %input_ptr, %input_base, %rd0;
    mul.wide.u32    %rd1, %thread_id, 32;
    add.u64         %output_ptr, %output_base, %rd1;
    
    // Initialize hash values
    mov.u32     %h0, 0x6a09e667;
    mov.u32     %h1, 0xbb67ae85;
    mov.u32     %h2, 0x3c6ef372;
    mov.u32     %h3, 0xa54ff53a;
    mov.u32     %h4, 0x510e527f;
    mov.u32     %h5, 0x9b05688c;
    mov.u32     %h6, 0x1f83d9ab;
    mov.u32     %h7, 0x5be0cd19;

    // Load 33-byte input as big-endian words
    ld.global.u8    %r4, [%input_ptr+0];
    ld.global.u8    %r5, [%input_ptr+1];
    ld.global.u8    %r6, [%input_ptr+2];
    ld.global.u8    %r7, [%input_ptr+3];
    shl.b32         %r4, %r4, 24;
    shl.b32         %r5, %r5, 16;
    shl.b32         %r6, %r6, 8;
    or.b32          %w0, %r4, %r5;
    or.b32          %w0, %w0, %r6;
    or.b32          %w0, %w0, %r7;
    ld.global.u8    %r4, [%input_ptr+4];
    ld.global.u8    %r5, [%input_ptr+5];
    ld.global.u8    %r6, [%input_ptr+6];
    ld.global.u8    %r7, [%input_ptr+7];
    shl.b32         %r4, %r4, 24;
    shl.b32         %r5, %r5, 16;
    shl.b32         %r6, %r6, 8;
    or.b32          %w1, %r4, %r5;
    or.b32          %w1, %w1, %r6;
    or.b32          %w1, %w1, %r7;
    ld.global.u8    %r4, [%input_ptr+8];
    ld.global.u8    %r5, [%input_ptr+9];
    ld.global.u8    %r6, [%input_ptr+10];
    ld.global.u8    %r7, [%input_ptr+11];
    shl.b32         %r4, %r4, 24;
    shl.b32         %r5, %r5, 16;
    shl.b32         %r6, %r6, 8;
    or.b32          %w2, %r4, %r5;
    or.b32          %w2, %w2, %r6;
    or.b32          %w2, %w2, %r7;
    ld.global.u8    %r4, [%input_ptr+12];
    ld.global.u8    %r5, [%input_ptr+13];
    ld.global.u8    %r6, [%input_ptr+14];
    ld.global.u8    %r7, [%input_ptr+15];
    shl.b32         %r4, %r4, 24;
    shl.b32         %r5, %r5, 16;
    shl.b32         %r6, %r6, 8;
    or.b32          %w3, %r4, %r5;
    or.b32          %w3, %w3, %r6;
    or.b32          %w3, %w3, %r7;
    ld.global.u8    %r4, [%input_ptr+16];
    ld.global.u8    %r5, [%input_ptr+17];
    ld.global.u8    %r6, [%input_ptr+18];
    ld.global.u8    %r7, [%input_ptr+19];
    shl.b32         %r4, %r4, 24;
    shl.b32         %r5, %r5, 16;
    shl.b32         %r6, %r6, 8;
    or.b32          %w4, %r4, %r5;
    or.b32          %w4, %w4, %r6;
    or.b32          %w4, %w4, %r7;
    ld.global.u8    %r4, [%input_ptr+20];
    ld.global.u8    %r5, [%input_ptr+21];
    ld.global.u8    %r6, [%input_ptr+22];
    ld.global.u8    %r7, [%input_ptr+23];
    shl.b32         %r4, %r4, 24;
    shl.b32         %r5, %r5, 16;
    shl.b32         %r6, %r6, 8;
    or.b32          %w5, %r4, %r5;
    or.b32          %w5, %w5, %r6;
    or.b32          %w5, %w5, %r7;
    ld.global.u8    %r4, [%input_ptr+24];
    ld.global.u8    %r5, [%input_ptr+25];
    ld.global.u8    %r6, [%input_ptr+26];
    ld.global.u8    %r7, [%input_ptr+27];
    shl.b32         %r4, %r4, 24;
    shl.b32         %r5, %r5, 16;
    shl.b32         %r6, %r6, 8;
    or.b32          %w6, %r4, %r5;
    or.b32          %w6, %w6, %r6;
    or.b32          %w6, %w6, %r7;
    ld.global.u8    %r4, [%input_ptr+28];
    ld.global.u8    %r5, [%input_ptr+29];
    ld.global.u8    %r6, [%input_ptr+30];
    ld.global.u8    %r7, [%input_ptr+31];
    shl.b32         %r4, %r4, 24;
    shl.b32         %r5, %r5, 16;
    shl.b32         %r6, %r6, 8;
    or.b32          %w7, %r4, %r5;
    or.b32          %w7, %w7, %r6;
    or.b32          %w7, %w7, %r7;
    ld.global.u8    %r4, [%input_ptr+32];
    shl.b32         %r4, %r4, 24;
    or.b32          %w8, %r4, 0x00800000;
    mov.u32         %w9, 0;
    mov.u32         %w10, 0;
    mov.u32         %w11, 0;
    mov.u32         %w12, 0;
    mov.u32         %w13, 0;
    mov.u32         %w14, 0;
    mov.u32         %w15, 0x00000108;
    
    // Initialize working variables
    mov.u32         %a, %h0;
    mov.u32         %b, %h1;
    mov.u32         %c, %h2;
    mov.u32         %d, %h3;
    mov.u32         %e, %h4;
    mov.u32         %f, %h5;
    mov.u32         %g, %h6;
    mov.u32         %h, %h7;

    // Round 0
    ld.const.u32    %k_val, [K+0];
    mov.u32         %w_val, %w0;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;


    // Round 1
    ld.const.u32    %k_val, [K+4];
    mov.u32         %w_val, %w1;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;


    // Round 2
    ld.const.u32    %k_val, [K+8];
    mov.u32         %w_val, %w2;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;


    // Round 3
    ld.const.u32    %k_val, [K+12];
    mov.u32         %w_val, %w3;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;


    // Round 4
    ld.const.u32    %k_val, [K+16];
    mov.u32         %w_val, %w4;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;


    // Round 5
    ld.const.u32    %k_val, [K+20];
    mov.u32         %w_val, %w5;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;


    // Round 6
    ld.const.u32    %k_val, [K+24];
    mov.u32         %w_val, %w6;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;


    // Round 7
    ld.const.u32    %k_val, [K+28];
    mov.u32         %w_val, %w7;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;


    // Round 8
    ld.const.u32    %k_val, [K+32];
    mov.u32         %w_val, %w8;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;


    // Round 9
    ld.const.u32    %k_val, [K+36];
    mov.u32         %w_val, %w9;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;


    // Round 10
    ld.const.u32    %k_val, [K+40];
    mov.u32         %w_val, %w10;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;


    // Round 11
    ld.const.u32    %k_val, [K+44];
    mov.u32         %w_val, %w11;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;


    // Round 12
    ld.const.u32    %k_val, [K+48];
    mov.u32         %w_val, %w12;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;


    // Round 13
    ld.const.u32    %k_val, [K+52];
    mov.u32         %w_val, %w13;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;


    // Round 14
    ld.const.u32    %k_val, [K+56];
    mov.u32         %w_val, %w14;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;


    // Round 15
    ld.const.u32    %k_val, [K+60];
    mov.u32         %w_val, %w15;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[16]
    // Save W[0] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w0;
    // sigma1(W[14])
    shr.u32         %r50, %w14, 17;
    shl.b32         %r51, %w14, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w14, 19;
    shl.b32         %r54, %w14, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w14, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[1])
    shr.u32         %r57, %w1, 7;
    shl.b32         %r58, %w1, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w1, 18;
    shl.b32         %r61, %w1, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w1, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[16] = sigma1 + W[9] + sigma0 + W[0]
    add.u32         %w0, %s1, %w9;
    add.u32         %w0, %w0, %s0;
    add.u32         %w0, %w0, %r70;

    // Round 16
    ld.const.u32    %k_val, [K+64];
    mov.u32         %w_val, %w0;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[17]
    // Save W[1] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w1;
    // sigma1(W[15])
    shr.u32         %r50, %w15, 17;
    shl.b32         %r51, %w15, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w15, 19;
    shl.b32         %r54, %w15, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w15, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[2])
    shr.u32         %r57, %w2, 7;
    shl.b32         %r58, %w2, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w2, 18;
    shl.b32         %r61, %w2, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w2, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[17] = sigma1 + W[10] + sigma0 + W[1]
    add.u32         %w1, %s1, %w10;
    add.u32         %w1, %w1, %s0;
    add.u32         %w1, %w1, %r70;

    // Round 17
    ld.const.u32    %k_val, [K+68];
    mov.u32         %w_val, %w1;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[18]
    // Save W[2] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w2;
    // sigma1(W[16])
    shr.u32         %r50, %w0, 17;
    shl.b32         %r51, %w0, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w0, 19;
    shl.b32         %r54, %w0, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w0, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[3])
    shr.u32         %r57, %w3, 7;
    shl.b32         %r58, %w3, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w3, 18;
    shl.b32         %r61, %w3, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w3, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[18] = sigma1 + W[11] + sigma0 + W[2]
    add.u32         %w2, %s1, %w11;
    add.u32         %w2, %w2, %s0;
    add.u32         %w2, %w2, %r70;

    // Round 18
    ld.const.u32    %k_val, [K+72];
    mov.u32         %w_val, %w2;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[19]
    // Save W[3] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w3;
    // sigma1(W[17])
    shr.u32         %r50, %w1, 17;
    shl.b32         %r51, %w1, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w1, 19;
    shl.b32         %r54, %w1, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w1, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[4])
    shr.u32         %r57, %w4, 7;
    shl.b32         %r58, %w4, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w4, 18;
    shl.b32         %r61, %w4, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w4, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[19] = sigma1 + W[12] + sigma0 + W[3]
    add.u32         %w3, %s1, %w12;
    add.u32         %w3, %w3, %s0;
    add.u32         %w3, %w3, %r70;

    // Round 19
    ld.const.u32    %k_val, [K+76];
    mov.u32         %w_val, %w3;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[20]
    // Save W[4] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w4;
    // sigma1(W[18])
    shr.u32         %r50, %w2, 17;
    shl.b32         %r51, %w2, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w2, 19;
    shl.b32         %r54, %w2, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w2, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[5])
    shr.u32         %r57, %w5, 7;
    shl.b32         %r58, %w5, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w5, 18;
    shl.b32         %r61, %w5, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w5, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[20] = sigma1 + W[13] + sigma0 + W[4]
    add.u32         %w4, %s1, %w13;
    add.u32         %w4, %w4, %s0;
    add.u32         %w4, %w4, %r70;

    // Round 20
    ld.const.u32    %k_val, [K+80];
    mov.u32         %w_val, %w4;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[21]
    // Save W[5] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w5;
    // sigma1(W[19])
    shr.u32         %r50, %w3, 17;
    shl.b32         %r51, %w3, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w3, 19;
    shl.b32         %r54, %w3, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w3, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[6])
    shr.u32         %r57, %w6, 7;
    shl.b32         %r58, %w6, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w6, 18;
    shl.b32         %r61, %w6, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w6, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[21] = sigma1 + W[14] + sigma0 + W[5]
    add.u32         %w5, %s1, %w14;
    add.u32         %w5, %w5, %s0;
    add.u32         %w5, %w5, %r70;

    // Round 21
    ld.const.u32    %k_val, [K+84];
    mov.u32         %w_val, %w5;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[22]
    // Save W[6] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w6;
    // sigma1(W[20])
    shr.u32         %r50, %w4, 17;
    shl.b32         %r51, %w4, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w4, 19;
    shl.b32         %r54, %w4, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w4, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[7])
    shr.u32         %r57, %w7, 7;
    shl.b32         %r58, %w7, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w7, 18;
    shl.b32         %r61, %w7, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w7, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[22] = sigma1 + W[15] + sigma0 + W[6]
    add.u32         %w6, %s1, %w15;
    add.u32         %w6, %w6, %s0;
    add.u32         %w6, %w6, %r70;

    // Round 22
    ld.const.u32    %k_val, [K+88];
    mov.u32         %w_val, %w6;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[23]
    // Save W[7] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w7;
    // sigma1(W[21])
    shr.u32         %r50, %w5, 17;
    shl.b32         %r51, %w5, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w5, 19;
    shl.b32         %r54, %w5, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w5, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[8])
    shr.u32         %r57, %w8, 7;
    shl.b32         %r58, %w8, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w8, 18;
    shl.b32         %r61, %w8, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w8, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[23] = sigma1 + W[16] + sigma0 + W[7]
    add.u32         %w7, %s1, %w0;
    add.u32         %w7, %w7, %s0;
    add.u32         %w7, %w7, %r70;

    // Round 23
    ld.const.u32    %k_val, [K+92];
    mov.u32         %w_val, %w7;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[24]
    // Save W[8] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w8;
    // sigma1(W[22])
    shr.u32         %r50, %w6, 17;
    shl.b32         %r51, %w6, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w6, 19;
    shl.b32         %r54, %w6, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w6, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[9])
    shr.u32         %r57, %w9, 7;
    shl.b32         %r58, %w9, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w9, 18;
    shl.b32         %r61, %w9, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w9, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[24] = sigma1 + W[17] + sigma0 + W[8]
    add.u32         %w8, %s1, %w1;
    add.u32         %w8, %w8, %s0;
    add.u32         %w8, %w8, %r70;

    // Round 24
    ld.const.u32    %k_val, [K+96];
    mov.u32         %w_val, %w8;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[25]
    // Save W[9] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w9;
    // sigma1(W[23])
    shr.u32         %r50, %w7, 17;
    shl.b32         %r51, %w7, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w7, 19;
    shl.b32         %r54, %w7, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w7, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[10])
    shr.u32         %r57, %w10, 7;
    shl.b32         %r58, %w10, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w10, 18;
    shl.b32         %r61, %w10, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w10, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[25] = sigma1 + W[18] + sigma0 + W[9]
    add.u32         %w9, %s1, %w2;
    add.u32         %w9, %w9, %s0;
    add.u32         %w9, %w9, %r70;

    // Round 25
    ld.const.u32    %k_val, [K+100];
    mov.u32         %w_val, %w9;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[26]
    // Save W[10] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w10;
    // sigma1(W[24])
    shr.u32         %r50, %w8, 17;
    shl.b32         %r51, %w8, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w8, 19;
    shl.b32         %r54, %w8, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w8, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[11])
    shr.u32         %r57, %w11, 7;
    shl.b32         %r58, %w11, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w11, 18;
    shl.b32         %r61, %w11, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w11, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[26] = sigma1 + W[19] + sigma0 + W[10]
    add.u32         %w10, %s1, %w3;
    add.u32         %w10, %w10, %s0;
    add.u32         %w10, %w10, %r70;

    // Round 26
    ld.const.u32    %k_val, [K+104];
    mov.u32         %w_val, %w10;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[27]
    // Save W[11] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w11;
    // sigma1(W[25])
    shr.u32         %r50, %w9, 17;
    shl.b32         %r51, %w9, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w9, 19;
    shl.b32         %r54, %w9, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w9, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[12])
    shr.u32         %r57, %w12, 7;
    shl.b32         %r58, %w12, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w12, 18;
    shl.b32         %r61, %w12, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w12, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[27] = sigma1 + W[20] + sigma0 + W[11]
    add.u32         %w11, %s1, %w4;
    add.u32         %w11, %w11, %s0;
    add.u32         %w11, %w11, %r70;

    // Round 27
    ld.const.u32    %k_val, [K+108];
    mov.u32         %w_val, %w11;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[28]
    // Save W[12] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w12;
    // sigma1(W[26])
    shr.u32         %r50, %w10, 17;
    shl.b32         %r51, %w10, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w10, 19;
    shl.b32         %r54, %w10, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w10, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[13])
    shr.u32         %r57, %w13, 7;
    shl.b32         %r58, %w13, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w13, 18;
    shl.b32         %r61, %w13, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w13, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[28] = sigma1 + W[21] + sigma0 + W[12]
    add.u32         %w12, %s1, %w5;
    add.u32         %w12, %w12, %s0;
    add.u32         %w12, %w12, %r70;

    // Round 28
    ld.const.u32    %k_val, [K+112];
    mov.u32         %w_val, %w12;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[29]
    // Save W[13] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w13;
    // sigma1(W[27])
    shr.u32         %r50, %w11, 17;
    shl.b32         %r51, %w11, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w11, 19;
    shl.b32         %r54, %w11, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w11, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[14])
    shr.u32         %r57, %w14, 7;
    shl.b32         %r58, %w14, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w14, 18;
    shl.b32         %r61, %w14, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w14, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[29] = sigma1 + W[22] + sigma0 + W[13]
    add.u32         %w13, %s1, %w6;
    add.u32         %w13, %w13, %s0;
    add.u32         %w13, %w13, %r70;

    // Round 29
    ld.const.u32    %k_val, [K+116];
    mov.u32         %w_val, %w13;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[30]
    // Save W[14] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w14;
    // sigma1(W[28])
    shr.u32         %r50, %w12, 17;
    shl.b32         %r51, %w12, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w12, 19;
    shl.b32         %r54, %w12, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w12, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[15])
    shr.u32         %r57, %w15, 7;
    shl.b32         %r58, %w15, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w15, 18;
    shl.b32         %r61, %w15, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w15, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[30] = sigma1 + W[23] + sigma0 + W[14]
    add.u32         %w14, %s1, %w7;
    add.u32         %w14, %w14, %s0;
    add.u32         %w14, %w14, %r70;

    // Round 30
    ld.const.u32    %k_val, [K+120];
    mov.u32         %w_val, %w14;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[31]
    // Save W[15] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w15;
    // sigma1(W[29])
    shr.u32         %r50, %w13, 17;
    shl.b32         %r51, %w13, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w13, 19;
    shl.b32         %r54, %w13, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w13, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[16])
    shr.u32         %r57, %w0, 7;
    shl.b32         %r58, %w0, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w0, 18;
    shl.b32         %r61, %w0, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w0, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[31] = sigma1 + W[24] + sigma0 + W[15]
    add.u32         %w15, %s1, %w8;
    add.u32         %w15, %w15, %s0;
    add.u32         %w15, %w15, %r70;

    // Round 31
    ld.const.u32    %k_val, [K+124];
    mov.u32         %w_val, %w15;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[32]
    // Save W[16] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w0;
    // sigma1(W[30])
    shr.u32         %r50, %w14, 17;
    shl.b32         %r51, %w14, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w14, 19;
    shl.b32         %r54, %w14, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w14, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[17])
    shr.u32         %r57, %w1, 7;
    shl.b32         %r58, %w1, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w1, 18;
    shl.b32         %r61, %w1, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w1, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[32] = sigma1 + W[25] + sigma0 + W[16]
    add.u32         %w0, %s1, %w9;
    add.u32         %w0, %w0, %s0;
    add.u32         %w0, %w0, %r70;

    // Round 32
    ld.const.u32    %k_val, [K+128];
    mov.u32         %w_val, %w0;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[33]
    // Save W[17] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w1;
    // sigma1(W[31])
    shr.u32         %r50, %w15, 17;
    shl.b32         %r51, %w15, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w15, 19;
    shl.b32         %r54, %w15, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w15, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[18])
    shr.u32         %r57, %w2, 7;
    shl.b32         %r58, %w2, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w2, 18;
    shl.b32         %r61, %w2, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w2, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[33] = sigma1 + W[26] + sigma0 + W[17]
    add.u32         %w1, %s1, %w10;
    add.u32         %w1, %w1, %s0;
    add.u32         %w1, %w1, %r70;

    // Round 33
    ld.const.u32    %k_val, [K+132];
    mov.u32         %w_val, %w1;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[34]
    // Save W[18] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w2;
    // sigma1(W[32])
    shr.u32         %r50, %w0, 17;
    shl.b32         %r51, %w0, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w0, 19;
    shl.b32         %r54, %w0, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w0, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[19])
    shr.u32         %r57, %w3, 7;
    shl.b32         %r58, %w3, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w3, 18;
    shl.b32         %r61, %w3, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w3, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[34] = sigma1 + W[27] + sigma0 + W[18]
    add.u32         %w2, %s1, %w11;
    add.u32         %w2, %w2, %s0;
    add.u32         %w2, %w2, %r70;

    // Round 34
    ld.const.u32    %k_val, [K+136];
    mov.u32         %w_val, %w2;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[35]
    // Save W[19] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w3;
    // sigma1(W[33])
    shr.u32         %r50, %w1, 17;
    shl.b32         %r51, %w1, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w1, 19;
    shl.b32         %r54, %w1, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w1, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[20])
    shr.u32         %r57, %w4, 7;
    shl.b32         %r58, %w4, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w4, 18;
    shl.b32         %r61, %w4, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w4, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[35] = sigma1 + W[28] + sigma0 + W[19]
    add.u32         %w3, %s1, %w12;
    add.u32         %w3, %w3, %s0;
    add.u32         %w3, %w3, %r70;

    // Round 35
    ld.const.u32    %k_val, [K+140];
    mov.u32         %w_val, %w3;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[36]
    // Save W[20] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w4;
    // sigma1(W[34])
    shr.u32         %r50, %w2, 17;
    shl.b32         %r51, %w2, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w2, 19;
    shl.b32         %r54, %w2, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w2, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[21])
    shr.u32         %r57, %w5, 7;
    shl.b32         %r58, %w5, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w5, 18;
    shl.b32         %r61, %w5, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w5, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[36] = sigma1 + W[29] + sigma0 + W[20]
    add.u32         %w4, %s1, %w13;
    add.u32         %w4, %w4, %s0;
    add.u32         %w4, %w4, %r70;

    // Round 36
    ld.const.u32    %k_val, [K+144];
    mov.u32         %w_val, %w4;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[37]
    // Save W[21] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w5;
    // sigma1(W[35])
    shr.u32         %r50, %w3, 17;
    shl.b32         %r51, %w3, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w3, 19;
    shl.b32         %r54, %w3, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w3, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[22])
    shr.u32         %r57, %w6, 7;
    shl.b32         %r58, %w6, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w6, 18;
    shl.b32         %r61, %w6, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w6, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[37] = sigma1 + W[30] + sigma0 + W[21]
    add.u32         %w5, %s1, %w14;
    add.u32         %w5, %w5, %s0;
    add.u32         %w5, %w5, %r70;

    // Round 37
    ld.const.u32    %k_val, [K+148];
    mov.u32         %w_val, %w5;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[38]
    // Save W[22] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w6;
    // sigma1(W[36])
    shr.u32         %r50, %w4, 17;
    shl.b32         %r51, %w4, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w4, 19;
    shl.b32         %r54, %w4, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w4, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[23])
    shr.u32         %r57, %w7, 7;
    shl.b32         %r58, %w7, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w7, 18;
    shl.b32         %r61, %w7, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w7, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[38] = sigma1 + W[31] + sigma0 + W[22]
    add.u32         %w6, %s1, %w15;
    add.u32         %w6, %w6, %s0;
    add.u32         %w6, %w6, %r70;

    // Round 38
    ld.const.u32    %k_val, [K+152];
    mov.u32         %w_val, %w6;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[39]
    // Save W[23] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w7;
    // sigma1(W[37])
    shr.u32         %r50, %w5, 17;
    shl.b32         %r51, %w5, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w5, 19;
    shl.b32         %r54, %w5, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w5, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[24])
    shr.u32         %r57, %w8, 7;
    shl.b32         %r58, %w8, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w8, 18;
    shl.b32         %r61, %w8, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w8, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[39] = sigma1 + W[32] + sigma0 + W[23]
    add.u32         %w7, %s1, %w0;
    add.u32         %w7, %w7, %s0;
    add.u32         %w7, %w7, %r70;

    // Round 39
    ld.const.u32    %k_val, [K+156];
    mov.u32         %w_val, %w7;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[40]
    // Save W[24] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w8;
    // sigma1(W[38])
    shr.u32         %r50, %w6, 17;
    shl.b32         %r51, %w6, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w6, 19;
    shl.b32         %r54, %w6, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w6, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[25])
    shr.u32         %r57, %w9, 7;
    shl.b32         %r58, %w9, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w9, 18;
    shl.b32         %r61, %w9, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w9, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[40] = sigma1 + W[33] + sigma0 + W[24]
    add.u32         %w8, %s1, %w1;
    add.u32         %w8, %w8, %s0;
    add.u32         %w8, %w8, %r70;

    // Round 40
    ld.const.u32    %k_val, [K+160];
    mov.u32         %w_val, %w8;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[41]
    // Save W[25] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w9;
    // sigma1(W[39])
    shr.u32         %r50, %w7, 17;
    shl.b32         %r51, %w7, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w7, 19;
    shl.b32         %r54, %w7, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w7, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[26])
    shr.u32         %r57, %w10, 7;
    shl.b32         %r58, %w10, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w10, 18;
    shl.b32         %r61, %w10, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w10, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[41] = sigma1 + W[34] + sigma0 + W[25]
    add.u32         %w9, %s1, %w2;
    add.u32         %w9, %w9, %s0;
    add.u32         %w9, %w9, %r70;

    // Round 41
    ld.const.u32    %k_val, [K+164];
    mov.u32         %w_val, %w9;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[42]
    // Save W[26] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w10;
    // sigma1(W[40])
    shr.u32         %r50, %w8, 17;
    shl.b32         %r51, %w8, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w8, 19;
    shl.b32         %r54, %w8, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w8, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[27])
    shr.u32         %r57, %w11, 7;
    shl.b32         %r58, %w11, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w11, 18;
    shl.b32         %r61, %w11, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w11, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[42] = sigma1 + W[35] + sigma0 + W[26]
    add.u32         %w10, %s1, %w3;
    add.u32         %w10, %w10, %s0;
    add.u32         %w10, %w10, %r70;

    // Round 42
    ld.const.u32    %k_val, [K+168];
    mov.u32         %w_val, %w10;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[43]
    // Save W[27] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w11;
    // sigma1(W[41])
    shr.u32         %r50, %w9, 17;
    shl.b32         %r51, %w9, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w9, 19;
    shl.b32         %r54, %w9, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w9, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[28])
    shr.u32         %r57, %w12, 7;
    shl.b32         %r58, %w12, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w12, 18;
    shl.b32         %r61, %w12, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w12, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[43] = sigma1 + W[36] + sigma0 + W[27]
    add.u32         %w11, %s1, %w4;
    add.u32         %w11, %w11, %s0;
    add.u32         %w11, %w11, %r70;

    // Round 43
    ld.const.u32    %k_val, [K+172];
    mov.u32         %w_val, %w11;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[44]
    // Save W[28] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w12;
    // sigma1(W[42])
    shr.u32         %r50, %w10, 17;
    shl.b32         %r51, %w10, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w10, 19;
    shl.b32         %r54, %w10, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w10, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[29])
    shr.u32         %r57, %w13, 7;
    shl.b32         %r58, %w13, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w13, 18;
    shl.b32         %r61, %w13, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w13, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[44] = sigma1 + W[37] + sigma0 + W[28]
    add.u32         %w12, %s1, %w5;
    add.u32         %w12, %w12, %s0;
    add.u32         %w12, %w12, %r70;

    // Round 44
    ld.const.u32    %k_val, [K+176];
    mov.u32         %w_val, %w12;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[45]
    // Save W[29] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w13;
    // sigma1(W[43])
    shr.u32         %r50, %w11, 17;
    shl.b32         %r51, %w11, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w11, 19;
    shl.b32         %r54, %w11, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w11, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[30])
    shr.u32         %r57, %w14, 7;
    shl.b32         %r58, %w14, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w14, 18;
    shl.b32         %r61, %w14, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w14, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[45] = sigma1 + W[38] + sigma0 + W[29]
    add.u32         %w13, %s1, %w6;
    add.u32         %w13, %w13, %s0;
    add.u32         %w13, %w13, %r70;

    // Round 45
    ld.const.u32    %k_val, [K+180];
    mov.u32         %w_val, %w13;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[46]
    // Save W[30] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w14;
    // sigma1(W[44])
    shr.u32         %r50, %w12, 17;
    shl.b32         %r51, %w12, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w12, 19;
    shl.b32         %r54, %w12, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w12, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[31])
    shr.u32         %r57, %w15, 7;
    shl.b32         %r58, %w15, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w15, 18;
    shl.b32         %r61, %w15, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w15, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[46] = sigma1 + W[39] + sigma0 + W[30]
    add.u32         %w14, %s1, %w7;
    add.u32         %w14, %w14, %s0;
    add.u32         %w14, %w14, %r70;

    // Round 46
    ld.const.u32    %k_val, [K+184];
    mov.u32         %w_val, %w14;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[47]
    // Save W[31] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w15;
    // sigma1(W[45])
    shr.u32         %r50, %w13, 17;
    shl.b32         %r51, %w13, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w13, 19;
    shl.b32         %r54, %w13, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w13, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[32])
    shr.u32         %r57, %w0, 7;
    shl.b32         %r58, %w0, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w0, 18;
    shl.b32         %r61, %w0, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w0, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[47] = sigma1 + W[40] + sigma0 + W[31]
    add.u32         %w15, %s1, %w8;
    add.u32         %w15, %w15, %s0;
    add.u32         %w15, %w15, %r70;

    // Round 47
    ld.const.u32    %k_val, [K+188];
    mov.u32         %w_val, %w15;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[48]
    // Save W[32] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w0;
    // sigma1(W[46])
    shr.u32         %r50, %w14, 17;
    shl.b32         %r51, %w14, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w14, 19;
    shl.b32         %r54, %w14, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w14, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[33])
    shr.u32         %r57, %w1, 7;
    shl.b32         %r58, %w1, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w1, 18;
    shl.b32         %r61, %w1, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w1, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[48] = sigma1 + W[41] + sigma0 + W[32]
    add.u32         %w0, %s1, %w9;
    add.u32         %w0, %w0, %s0;
    add.u32         %w0, %w0, %r70;

    // Round 48
    ld.const.u32    %k_val, [K+192];
    mov.u32         %w_val, %w0;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[49]
    // Save W[33] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w1;
    // sigma1(W[47])
    shr.u32         %r50, %w15, 17;
    shl.b32         %r51, %w15, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w15, 19;
    shl.b32         %r54, %w15, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w15, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[34])
    shr.u32         %r57, %w2, 7;
    shl.b32         %r58, %w2, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w2, 18;
    shl.b32         %r61, %w2, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w2, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[49] = sigma1 + W[42] + sigma0 + W[33]
    add.u32         %w1, %s1, %w10;
    add.u32         %w1, %w1, %s0;
    add.u32         %w1, %w1, %r70;

    // Round 49
    ld.const.u32    %k_val, [K+196];
    mov.u32         %w_val, %w1;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[50]
    // Save W[34] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w2;
    // sigma1(W[48])
    shr.u32         %r50, %w0, 17;
    shl.b32         %r51, %w0, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w0, 19;
    shl.b32         %r54, %w0, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w0, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[35])
    shr.u32         %r57, %w3, 7;
    shl.b32         %r58, %w3, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w3, 18;
    shl.b32         %r61, %w3, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w3, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[50] = sigma1 + W[43] + sigma0 + W[34]
    add.u32         %w2, %s1, %w11;
    add.u32         %w2, %w2, %s0;
    add.u32         %w2, %w2, %r70;

    // Round 50
    ld.const.u32    %k_val, [K+200];
    mov.u32         %w_val, %w2;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[51]
    // Save W[35] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w3;
    // sigma1(W[49])
    shr.u32         %r50, %w1, 17;
    shl.b32         %r51, %w1, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w1, 19;
    shl.b32         %r54, %w1, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w1, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[36])
    shr.u32         %r57, %w4, 7;
    shl.b32         %r58, %w4, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w4, 18;
    shl.b32         %r61, %w4, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w4, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[51] = sigma1 + W[44] + sigma0 + W[35]
    add.u32         %w3, %s1, %w12;
    add.u32         %w3, %w3, %s0;
    add.u32         %w3, %w3, %r70;

    // Round 51
    ld.const.u32    %k_val, [K+204];
    mov.u32         %w_val, %w3;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[52]
    // Save W[36] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w4;
    // sigma1(W[50])
    shr.u32         %r50, %w2, 17;
    shl.b32         %r51, %w2, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w2, 19;
    shl.b32         %r54, %w2, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w2, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[37])
    shr.u32         %r57, %w5, 7;
    shl.b32         %r58, %w5, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w5, 18;
    shl.b32         %r61, %w5, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w5, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[52] = sigma1 + W[45] + sigma0 + W[36]
    add.u32         %w4, %s1, %w13;
    add.u32         %w4, %w4, %s0;
    add.u32         %w4, %w4, %r70;

    // Round 52
    ld.const.u32    %k_val, [K+208];
    mov.u32         %w_val, %w4;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[53]
    // Save W[37] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w5;
    // sigma1(W[51])
    shr.u32         %r50, %w3, 17;
    shl.b32         %r51, %w3, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w3, 19;
    shl.b32         %r54, %w3, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w3, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[38])
    shr.u32         %r57, %w6, 7;
    shl.b32         %r58, %w6, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w6, 18;
    shl.b32         %r61, %w6, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w6, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[53] = sigma1 + W[46] + sigma0 + W[37]
    add.u32         %w5, %s1, %w14;
    add.u32         %w5, %w5, %s0;
    add.u32         %w5, %w5, %r70;

    // Round 53
    ld.const.u32    %k_val, [K+212];
    mov.u32         %w_val, %w5;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[54]
    // Save W[38] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w6;
    // sigma1(W[52])
    shr.u32         %r50, %w4, 17;
    shl.b32         %r51, %w4, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w4, 19;
    shl.b32         %r54, %w4, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w4, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[39])
    shr.u32         %r57, %w7, 7;
    shl.b32         %r58, %w7, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w7, 18;
    shl.b32         %r61, %w7, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w7, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[54] = sigma1 + W[47] + sigma0 + W[38]
    add.u32         %w6, %s1, %w15;
    add.u32         %w6, %w6, %s0;
    add.u32         %w6, %w6, %r70;

    // Round 54
    ld.const.u32    %k_val, [K+216];
    mov.u32         %w_val, %w6;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[55]
    // Save W[39] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w7;
    // sigma1(W[53])
    shr.u32         %r50, %w5, 17;
    shl.b32         %r51, %w5, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w5, 19;
    shl.b32         %r54, %w5, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w5, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[40])
    shr.u32         %r57, %w8, 7;
    shl.b32         %r58, %w8, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w8, 18;
    shl.b32         %r61, %w8, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w8, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[55] = sigma1 + W[48] + sigma0 + W[39]
    add.u32         %w7, %s1, %w0;
    add.u32         %w7, %w7, %s0;
    add.u32         %w7, %w7, %r70;

    // Round 55
    ld.const.u32    %k_val, [K+220];
    mov.u32         %w_val, %w7;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[56]
    // Save W[40] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w8;
    // sigma1(W[54])
    shr.u32         %r50, %w6, 17;
    shl.b32         %r51, %w6, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w6, 19;
    shl.b32         %r54, %w6, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w6, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[41])
    shr.u32         %r57, %w9, 7;
    shl.b32         %r58, %w9, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w9, 18;
    shl.b32         %r61, %w9, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w9, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[56] = sigma1 + W[49] + sigma0 + W[40]
    add.u32         %w8, %s1, %w1;
    add.u32         %w8, %w8, %s0;
    add.u32         %w8, %w8, %r70;

    // Round 56
    ld.const.u32    %k_val, [K+224];
    mov.u32         %w_val, %w8;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[57]
    // Save W[41] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w9;
    // sigma1(W[55])
    shr.u32         %r50, %w7, 17;
    shl.b32         %r51, %w7, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w7, 19;
    shl.b32         %r54, %w7, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w7, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[42])
    shr.u32         %r57, %w10, 7;
    shl.b32         %r58, %w10, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w10, 18;
    shl.b32         %r61, %w10, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w10, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[57] = sigma1 + W[50] + sigma0 + W[41]
    add.u32         %w9, %s1, %w2;
    add.u32         %w9, %w9, %s0;
    add.u32         %w9, %w9, %r70;

    // Round 57
    ld.const.u32    %k_val, [K+228];
    mov.u32         %w_val, %w9;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[58]
    // Save W[42] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w10;
    // sigma1(W[56])
    shr.u32         %r50, %w8, 17;
    shl.b32         %r51, %w8, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w8, 19;
    shl.b32         %r54, %w8, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w8, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[43])
    shr.u32         %r57, %w11, 7;
    shl.b32         %r58, %w11, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w11, 18;
    shl.b32         %r61, %w11, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w11, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[58] = sigma1 + W[51] + sigma0 + W[42]
    add.u32         %w10, %s1, %w3;
    add.u32         %w10, %w10, %s0;
    add.u32         %w10, %w10, %r70;

    // Round 58
    ld.const.u32    %k_val, [K+232];
    mov.u32         %w_val, %w10;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[59]
    // Save W[43] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w11;
    // sigma1(W[57])
    shr.u32         %r50, %w9, 17;
    shl.b32         %r51, %w9, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w9, 19;
    shl.b32         %r54, %w9, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w9, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[44])
    shr.u32         %r57, %w12, 7;
    shl.b32         %r58, %w12, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w12, 18;
    shl.b32         %r61, %w12, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w12, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[59] = sigma1 + W[52] + sigma0 + W[43]
    add.u32         %w11, %s1, %w4;
    add.u32         %w11, %w11, %s0;
    add.u32         %w11, %w11, %r70;

    // Round 59
    ld.const.u32    %k_val, [K+236];
    mov.u32         %w_val, %w11;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[60]
    // Save W[44] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w12;
    // sigma1(W[58])
    shr.u32         %r50, %w10, 17;
    shl.b32         %r51, %w10, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w10, 19;
    shl.b32         %r54, %w10, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w10, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[45])
    shr.u32         %r57, %w13, 7;
    shl.b32         %r58, %w13, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w13, 18;
    shl.b32         %r61, %w13, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w13, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[60] = sigma1 + W[53] + sigma0 + W[44]
    add.u32         %w12, %s1, %w5;
    add.u32         %w12, %w12, %s0;
    add.u32         %w12, %w12, %r70;

    // Round 60
    ld.const.u32    %k_val, [K+240];
    mov.u32         %w_val, %w12;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[61]
    // Save W[45] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w13;
    // sigma1(W[59])
    shr.u32         %r50, %w11, 17;
    shl.b32         %r51, %w11, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w11, 19;
    shl.b32         %r54, %w11, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w11, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[46])
    shr.u32         %r57, %w14, 7;
    shl.b32         %r58, %w14, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w14, 18;
    shl.b32         %r61, %w14, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w14, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[61] = sigma1 + W[54] + sigma0 + W[45]
    add.u32         %w13, %s1, %w6;
    add.u32         %w13, %w13, %s0;
    add.u32         %w13, %w13, %r70;

    // Round 61
    ld.const.u32    %k_val, [K+244];
    mov.u32         %w_val, %w13;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[62]
    // Save W[46] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w14;
    // sigma1(W[60])
    shr.u32         %r50, %w12, 17;
    shl.b32         %r51, %w12, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w12, 19;
    shl.b32         %r54, %w12, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w12, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[47])
    shr.u32         %r57, %w15, 7;
    shl.b32         %r58, %w15, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w15, 18;
    shl.b32         %r61, %w15, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w15, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[62] = sigma1 + W[55] + sigma0 + W[46]
    add.u32         %w14, %s1, %w7;
    add.u32         %w14, %w14, %s0;
    add.u32         %w14, %w14, %r70;

    // Round 62
    ld.const.u32    %k_val, [K+248];
    mov.u32         %w_val, %w14;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Extend W[63]
    // Save W[47] before we overwrite it (if w_i == w_i_16)
    mov.u32         %r70, %w15;
    // sigma1(W[61])
    shr.u32         %r50, %w13, 17;
    shl.b32         %r51, %w13, 15;
    or.b32          %r52, %r50, %r51;
    shr.u32         %r53, %w13, 19;
    shl.b32         %r54, %w13, 13;
    or.b32          %r55, %r53, %r54;
    shr.u32         %r56, %w13, 10;
    xor.b32         %s1, %r52, %r55;
    xor.b32         %s1, %s1, %r56;
    // sigma0(W[48])
    shr.u32         %r57, %w0, 7;
    shl.b32         %r58, %w0, 25;
    or.b32          %r59, %r57, %r58;
    shr.u32         %r60, %w0, 18;
    shl.b32         %r61, %w0, 14;
    or.b32          %r62, %r60, %r61;
    shr.u32         %r63, %w0, 3;
    xor.b32         %s0, %r59, %r62;
    xor.b32         %s0, %s0, %r63;
    // W[63] = sigma1 + W[56] + sigma0 + W[47]
    add.u32         %w15, %s1, %w8;
    add.u32         %w15, %w15, %s0;
    add.u32         %w15, %w15, %r70;

    // Round 63
    ld.const.u32    %k_val, [K+252];
    mov.u32         %w_val, %w15;
    // Ch(e,f,g) = (e & f) ^ (~e & g)
    and.b32         %r10, %e, %f;
    not.b32         %r11, %e;
    and.b32         %r11, %r11, %g;
    xor.b32         %ch, %r10, %r11;
    // Maj(a,b,c) = (a & b) ^ (a & c) ^ (b & c)
    and.b32         %r12, %a, %b;
    and.b32         %r13, %a, %c;
    and.b32         %r14, %b, %c;
    xor.b32         %maj, %r12, %r13;
    xor.b32         %maj, %maj, %r14;
    // Sigma1(e) = ROTR(e,6) ^ ROTR(e,11) ^ ROTR(e,25)
    shr.u32         %r15, %e, 6;
    shl.b32         %r16, %e, 26;
    or.b32          %r17, %r15, %r16;
    shr.u32         %r18, %e, 11;
    shl.b32         %r19, %e, 21;
    or.b32          %r20, %r18, %r19;
    shr.u32         %r21, %e, 25;
    shl.b32         %r22, %e, 7;
    or.b32          %r23, %r21, %r22;
    xor.b32         %S1, %r17, %r20;
    xor.b32         %S1, %S1, %r23;
    // Sigma0(a) = ROTR(a,2) ^ ROTR(a,13) ^ ROTR(a,22)
    shr.u32         %r24, %a, 2;
    shl.b32         %r25, %a, 30;
    or.b32          %r26, %r24, %r25;
    shr.u32         %r27, %a, 13;
    shl.b32         %r28, %a, 19;
    or.b32          %r29, %r27, %r28;
    shr.u32         %r30, %a, 22;
    shl.b32         %r31, %a, 10;
    or.b32          %r32, %r30, %r31;
    xor.b32         %S0, %r26, %r29;
    xor.b32         %S0, %S0, %r32;
    // t1 = h + Sigma1 + ch + k + w
    add.u32         %t1, %h, %S1;
    add.u32         %t1, %t1, %ch;
    add.u32         %t1, %t1, %k_val;
    add.u32         %t1, %t1, %w_val;
    // t2 = Sigma0 + maj
    add.u32         %t2, %S0, %maj;
    // Update working variables
    mov.u32         %h, %g;
    mov.u32         %g, %f;
    mov.u32         %f, %e;
    add.u32         %e, %d, %t1;
    mov.u32         %d, %c;
    mov.u32         %c, %b;
    mov.u32         %b, %a;
    add.u32         %a, %t1, %t2;

    // Add compressed hash to initial values
    add.u32         %h0, %h0, %a;
    add.u32         %h1, %h1, %b;
    add.u32         %h2, %h2, %c;
    add.u32         %h3, %h3, %d;
    add.u32         %h4, %h4, %e;
    add.u32         %h5, %h5, %f;
    add.u32         %h6, %h6, %g;
    add.u32         %h7, %h7, %h;
    
    // Store output as big-endian bytes
    shr.u32         %r40, %h0, 24;
    st.global.u8    [%output_ptr+0], %r40;
    shr.u32         %r41, %h0, 16;
    st.global.u8    [%output_ptr+1], %r41;
    shr.u32         %r42, %h0, 8;
    st.global.u8    [%output_ptr+2], %r42;
    st.global.u8    [%output_ptr+3], %h0;
    shr.u32         %r40, %h1, 24;
    st.global.u8    [%output_ptr+4], %r40;
    shr.u32         %r41, %h1, 16;
    st.global.u8    [%output_ptr+5], %r41;
    shr.u32         %r42, %h1, 8;
    st.global.u8    [%output_ptr+6], %r42;
    st.global.u8    [%output_ptr+7], %h1;
    shr.u32         %r40, %h2, 24;
    st.global.u8    [%output_ptr+8], %r40;
    shr.u32         %r41, %h2, 16;
    st.global.u8    [%output_ptr+9], %r41;
    shr.u32         %r42, %h2, 8;
    st.global.u8    [%output_ptr+10], %r42;
    st.global.u8    [%output_ptr+11], %h2;
    shr.u32         %r40, %h3, 24;
    st.global.u8    [%output_ptr+12], %r40;
    shr.u32         %r41, %h3, 16;
    st.global.u8    [%output_ptr+13], %r41;
    shr.u32         %r42, %h3, 8;
    st.global.u8    [%output_ptr+14], %r42;
    st.global.u8    [%output_ptr+15], %h3;
    shr.u32         %r40, %h4, 24;
    st.global.u8    [%output_ptr+16], %r40;
    shr.u32         %r41, %h4, 16;
    st.global.u8    [%output_ptr+17], %r41;
    shr.u32         %r42, %h4, 8;
    st.global.u8    [%output_ptr+18], %r42;
    st.global.u8    [%output_ptr+19], %h4;
    shr.u32         %r40, %h5, 24;
    st.global.u8    [%output_ptr+20], %r40;
    shr.u32         %r41, %h5, 16;
    st.global.u8    [%output_ptr+21], %r41;
    shr.u32         %r42, %h5, 8;
    st.global.u8    [%output_ptr+22], %r42;
    st.global.u8    [%output_ptr+23], %h5;
    shr.u32         %r40, %h6, 24;
    st.global.u8    [%output_ptr+24], %r40;
    shr.u32         %r41, %h6, 16;
    st.global.u8    [%output_ptr+25], %r41;
    shr.u32         %r42, %h6, 8;
    st.global.u8    [%output_ptr+26], %r42;
    st.global.u8    [%output_ptr+27], %h6;
    shr.u32         %r40, %h7, 24;
    st.global.u8    [%output_ptr+28], %r40;
    shr.u32         %r41, %h7, 16;
    st.global.u8    [%output_ptr+29], %r41;
    shr.u32         %r42, %h7, 8;
    st.global.u8    [%output_ptr+30], %r42;
    st.global.u8    [%output_ptr+31], %h7;

END:
    ret;
}
